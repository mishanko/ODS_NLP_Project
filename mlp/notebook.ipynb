{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from preprocess import RegexPreprocessor\n",
    "\n",
    "\n",
    "datadir = Path(\"/data/guesslang_data/Dataset/Data/train\")\n",
    "files = list(datadir.glob(\"*\"))\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'apple': 4, 'banana': 4, 'cherry': 4})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Пример списков Counter\n",
    "counters = [\n",
    "    Counter({'apple': 3, 'banana': 1}),\n",
    "    Counter({'apple': 1, 'banana': 2, 'cherry': 1}),\n",
    "    Counter({'banana': 1, 'cherry': 3})\n",
    "]\n",
    "\n",
    "# Сложение списков Counter\n",
    "total_counter = sum(counters, Counter())\n",
    "\n",
    "print(total_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = read_file(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      4\u001b[0m counter \u001b[38;5;241m=\u001b[39m CountVectorizer(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#$\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m()*+,-./:;<=>?@\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m]^_`\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m|}~]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1328\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1328\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1374\u001b[0m             )\n\u001b[1;32m   1375\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1380\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:-1\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "counter = CountVectorizer(\n",
    "    input=\"filename\",\n",
    "    token_pattern=r'\\w+|[!\\\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~]',\n",
    "    min_df=0.01,\n",
    ")\n",
    "counter.fit(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'include',\n",
       " '<',\n",
       " 'iostream',\n",
       " '>',\n",
       " 'using',\n",
       " 'namespace',\n",
       " 'std',\n",
       " ';',\n",
       " '/',\n",
       " '/',\n",
       " 'comment',\n",
       " 'int',\n",
       " 'main',\n",
       " '(',\n",
       " ')',\n",
       " '{',\n",
       " 'int',\n",
       " 'a',\n",
       " '=',\n",
       " '5',\n",
       " ',',\n",
       " 'b',\n",
       " '=',\n",
       " '10',\n",
       " ';',\n",
       " 'cout',\n",
       " '<',\n",
       " '<',\n",
       " '\"',\n",
       " 'sum',\n",
       " ':',\n",
       " '\"',\n",
       " '<',\n",
       " '<',\n",
       " '(',\n",
       " 'a',\n",
       " '+',\n",
       " 'b',\n",
       " ')',\n",
       " '<',\n",
       " '<',\n",
       " 'endl',\n",
       " ';',\n",
       " 'return',\n",
       " '0',\n",
       " ';',\n",
       " '}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = RegexPreprocessor()\n",
    "\n",
    "c_code = \"\"\"\n",
    "#include <stdio.h>\n",
    "\n",
    "// akld\n",
    "\n",
    "int main() {\n",
    "    int a = 5, b = 10;\n",
    "    printf(\"Sum: %d\\\\n\", a + b);\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Example C++ code\n",
    "cpp_code = \"\"\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// comment\n",
    "\n",
    "int main() {\n",
    "    int a = 5, b = 10;\n",
    "    cout << \"Sum: \" << (a + b) << endl;\n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "preprocessor(cpp_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
