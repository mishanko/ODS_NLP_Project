{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "exp_dir = Path(\"experiments/mlp_tfidf/runs/2024-05-19-17-25-58\")\n",
    "with open(exp_dir / \"models/pipeline.pkl\", \"rb\") as f:\n",
    "    pipeline = pickle.load(f)\n",
    "with open(exp_dir / \"models/label_encoder.pkl\", \"rb\") as f:\n",
    "    le = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/dmitry.zarubin/ODS_NLP_Project/classic/experiments/logreg_tfidf/runs/2024-05-19-06-20-20/models/pipeline.pkl\", \"rb\") as file:\n",
    "    vectorizer = pickle.load(file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=2048, min_df=0.05, ngram_range=(1, 3),\n",
       "                token_pattern=&#x27;\\\\b[a-zA-Z]+\\\\b|[(){}&lt;&gt;,;.:=\\\\[\\\\]]&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=2048, min_df=0.05, ngram_range=(1, 3),\n",
       "                token_pattern=&#x27;\\\\b[a-zA-Z]+\\\\b|[(){}&lt;&gt;,;.:=\\\\[\\\\]]&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=2048, min_df=0.05, ngram_range=(1, 3),\n",
       "                token_pattern='\\\\b[a-zA-Z]+\\\\b|[(){}<>,;.:=\\\\[\\\\]]')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def', 'foo', '(', ')', ':', 'return']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall(vectorizer.token_pattern, \"def foo(): return 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = {v:k for k, v in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the same'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv[601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def tokenize_from_vectorizer(\n",
    "    string: str, \n",
    "    vectorizer: TfidfVectorizer,\n",
    "    pad_value: int,\n",
    "    max_len: int = 128,\n",
    "):\n",
    "    tokens = re.findall(vectorizer.token_pattern, string)\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    tokens = [vocab[token] for token in tokens if token in vocab]\n",
    "    pad = max_len - len(tokens)\n",
    "    if pad > 0:\n",
    "        tokens += [pad_value] * pad\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class CodeClassificationMLP(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        num_classes: int,\n",
    "        hidden_dims: list[int] = [128],\n",
    "        act_fn: str = \"relu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.add_module(\"embedding\", nn.Embedding(vocab_size, embedding_dim, padding_idx))\n",
    "        act_fn = nn.ReLU() if act_fn == \"relu\" else nn.Identity\n",
    "        prev_dim = embedding_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            self.add_module(f\"layer_{i}\", nn.Linear(prev_dim, hidden_dim))\n",
    "            self.add_module(f\"act_fn_{i}\", act_fn)\n",
    "            prev_dim = hidden_dim\n",
    "        self.add_module(\"pool\", nn.AdaptiveAvgPool1d(1))\n",
    "        self.add_module(\"flatten\", nn.Flatten())\n",
    "        self.add_module(\"classifier\", nn.Linear(hidden_dim, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_from_vectorizer(\"def foo(): return 0\", vectorizer, max(vectorizer.vocabulary_.values()) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeClassificationMLP(\n",
       "  (embedding): Embedding(711, 32, padding_idx=710)\n",
       "  (layer_0): Linear(in_features=32, out_features=128, bias=True)\n",
       "  (act_fn_0): ReLU()\n",
       "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (classifier): Linear(in_features=128, out_features=54, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CodeClassificationMLP(711, 32, 710, 54, [128])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 54])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model(torch.tensor(tokens)[None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(vectorizer.vocabulary_.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LinearNormAct(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.add_module(\"linear\", nn.Linear(in_features, out_features))\n",
    "        self.add_module(\"norm\", nn.LayerNorm(out_features))\n",
    "        self.add_module(\"act\", nn.ReLU())\n",
    "\n",
    "\n",
    "class ConvNormAct(nn.Sequential):\n",
    "    def __init__(self, in_features: int, out_features: int, kernel_size: int, groups: int = 1):\n",
    "        super().__init__()\n",
    "        self.add_module(\"conv\", nn.Conv1d(in_features, out_features, kernel_size, groups, padding=kernel_size // 2))\n",
    "        self.add_module(\"norm\", nn.BatchNorm1d(out_features))\n",
    "        self.add_module(\"act\", nn.ReLU())\n",
    "\n",
    "\n",
    "class CodeClassificationMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        num_classes: int,\n",
    "        hidden_dims: list[int] = [256],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.layers = nn.Sequential()\n",
    "        prev_dim = embedding_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            self.layers.add_module(f\"block_{i}\", LinearNormAct(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(self.embedding(x))\n",
    "        x = self.pool(x.movedim(1, -1)).squeeze(-1)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "\n",
    "class CodeClassificationCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        num_classes: int,\n",
    "        hidden_dims: list[int] = [256],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.layers = nn.Sequential()\n",
    "        prev_dim = embedding_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            self.layers.add_module(f\"block_{i}\", ConvNormAct(prev_dim, hidden_dim, 5))\n",
    "            prev_dim = hidden_dim\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).movedim(1, -1)\n",
    "        x = self.layers(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([334,   0,  40, 148, 627, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811, 811,\n",
       "        811, 811])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tokens = torch.tensor(\n",
    "    tokenize_from_vectorizer(\"def foo(): return 0\", vectorizer, max(vectorizer.vocabulary_.values()) + 1)\n",
    ")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 54])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CodeClassificationCNN(812, 32, 811, 54, [128, 779])\n",
    "model(tokens[None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ModuleList(\n",
       "   (0): LinearNormAct(\n",
       "     (linear): Linear(in_features=32, out_features=256, bias=True)\n",
       "     (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "     (act): ReLU()\n",
       "   )\n",
       " ),\n",
       " Linear(in_features=256, out_features=54, bias=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers, model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
